import { z } from "zod";
import { log } from "../logger.js";
import { ChatMessage, IAnonymizer, IPrompt, ILlmProvider, ToolDefinition, ToolCall, IStreamingDeanonymizer, IHttpClient, IHttpResponse } from "../types.js";
import { ToolCallSchema } from "../schemas.js";

/**
 * Represents the response from a non-streaming OpenAI-compatible completion request.
 */
const OpenAIResponseSchema = z.object({
  /**
   * The list of completion choices returned by the model.
   */
  choices: z.array(
    z.object({
      /**
       * The message generated by the assistant.
       */
      message: z.object({
        /**
         * The text content of the message.
         */
        content: z.string().optional().nullable(),
        /**
         * Any tool calls requested by the model.
         */
        tool_calls: z.array(ToolCallSchema).nullish(),
      }).nullish(),
    })
  ).optional(),
  /**
   * An error object returned by the API if the request failed.
   */
  error: z.object({
    message: z.string().optional(),
    type: z.string().optional(),
    code: z.union([z.string(), z.number()]).optional(),
  }).optional(),
});

/**
 * Represents a tool call in a streaming response.
 */
const OpenAIStreamingToolCallSchema = z.object({
  /**
   * The index of the tool call in the response array.
   */
  index: z.number(),
  /**
   * The unique identifier for the tool call.
   */
  id: z.string().optional().nullable(),
  /**
   * The type of the tool call, typically "function".
   */
  type: z.literal("function").optional().nullable(),
  /**
   * The function call details, which may be streamed in parts.
   */
  function: z.object({
    /**
     * The name of the function to be called.
     */
    name: z.string().optional().nullable(),
    /**
     * The arguments for the function, as a JSON string that may be incomplete.
     */
    arguments: z.string().optional().nullable(),
  }).nullish(),
});

/**
 * Represents the incremental change (delta) in a streaming response chunk.
 */
const OpenAIStreamDeltaSchema = z.object({
  /**
   * A partial piece of the assistant's content message.
   */
  content: z.string().optional().nullable(),
  /**
   * Partial updates to tool calls being generated by the model.
   */
  tool_calls: z.array(OpenAIStreamingToolCallSchema).nullish(),
});

type OpenAIStreamDelta = z.infer<typeof OpenAIStreamDeltaSchema>;

/**
 * Represents a single chunk received from an OpenAI-compatible SSE stream.
 */
const OpenAIStreamChunkSchema = z.object({
  /**
   * The list of choices, each containing a delta update.  
   */
  choices: z.array(
    z.object({
      /**
       * The partial update for this choice.
       */
      delta: OpenAIStreamDeltaSchema.nullish(),
    })
  ).nullish(),
});

/**
 * Represents the body of a completion request sent to an OpenAI-compatible API.
 */
type OpenAIRequestBody = {
  /**
   * The ID of the model to use for the request.
   */
  model: string;
  /**
   * The conversation history to send to the model.
   */
  messages: ChatMessage[];
  /**
   * The maximum number of tokens to generate in the completion.
   */
  max_tokens: number;
  /**
   * Whether to stream back partial progress.
   */
  stream?: boolean;
  /**
   * A list of tools the model may call.
   */
  tools?: {
    /**
     * The type of the tool.
     */
    type: "function";
    /**
     * The definition of the function tool.
     */
    function: ToolDefinition;
  }[];
};

/**
 * Arguments for the OpenAICompatibleProvider constructor.
 */
export interface IOpenAICompatibleProviderArgs {
  /** The HTTP client to use for requests. */
  httpClient: IHttpClient;
  /** The API endpoint URL for the OpenAI-compatible service. */
  endpoint: string;
  /** The API key for authentication. */
  apiKey: string;
  /** The model identifier to be used for completions. */
  model: string;
  /** Optional anonymizer to protect sensitive data in user messages. */
  anonymizer?: IAnonymizer;
}

export class OpenAICompatibleProvider implements ILlmProvider {
  private httpClient: IHttpClient;
  private endpoint: string;
  private apiKey: string;
  private model: string;
  private anonymizer?: IAnonymizer;

  /**
   * Creates an instance of OpenAICompatibleProvider.
   * 
   * @param args - The configuration arguments for the provider.
   */
  constructor({
    httpClient,
    endpoint,
    apiKey,
    model,
    anonymizer,
  }: IOpenAICompatibleProviderArgs) {
    this.httpClient = httpClient;
    this.endpoint = endpoint;
    this.apiKey = apiKey;
    this.model = model;
    this.anonymizer = anonymizer;
  }

  /**
   * Prepares chat messages for the API request, applying anonymization to user messages if configured.
   * 
   * @param conversation - The list of chat messages in the conversation history.
   * @returns An array of message objects formatted for the OpenAI-compatible API.
   */
  private prepareMessages(
    conversation: ChatMessage[]
  ): ChatMessage[] {
    return conversation.map((message) => {
      const role = message.role;

      let content = message.content;
      if (this.anonymizer && message.role === "user") {
        content = this.anonymizer.anonymize(message.content);
      }

      const msg: ChatMessage = {
        role,
        content,
        tool_calls: message.tool_calls,
        tool_call_id: message.tool_call_id
      };
      return msg;
    });
  }

  /**
   * Constructs the request body for the completion API.
   * 
   * @param prompt - The prompt object that generates the chat history.
   * @param tools - Optional tool definitions for function calling.
   * @param stream - Whether the response should be streamed.
   * @returns A JSON-serializable object representing the request body.
   */
  private createRequestBody(
    prompt: IPrompt,
    tools: ToolDefinition[] | undefined,
    stream: boolean
  ): OpenAIRequestBody {
    const conversation = prompt.generateChatHistory();
    const messages = this.prepareMessages(conversation);

    const body: OpenAIRequestBody = {
      model: this.model,
      messages: messages,
      max_tokens: 10000,
    };

    if (stream) {
      body.stream = true;
    }

    if (tools && tools.length > 0) {
      body.tools = tools.map((t) => ({ type: "function", function: t }));
    }

    return body;
  }

  /**
   * Sends a POST request to the provider's endpoint using the injected httpClient.
   * 
   * @param body - The request body to be sent as JSON.
   * @param signal - Optional AbortSignal to cancel the request.
   * @returns A promise that resolves to the IHttpResponse object.
   */
  private async post(body: OpenAIRequestBody, signal?: AbortSignal): Promise<IHttpResponse> {
    return await this.httpClient.post(this.endpoint, {
      headers: {
        Authorization: `Bearer ${this.apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
      signal,
    });
  }

  /**
   * Performs a non-streaming completion request.
   * 
   * @param prompt - The prompt to be sent.
   * @param tools - Optional tools available for the model to use.
   * @param signal - Optional AbortSignal to cancel the request.
   * @returns A promise resolving to the assistant's message, or null if no choice was returned.
   */
  async query(
    prompt: IPrompt,
    tools?: ToolDefinition[],
    signal?: AbortSignal
  ): Promise<ChatMessage | null> {
    const body = this.createRequestBody(prompt, tools, false);
    const response = await this.post(body, signal);

    let rawJson: any;
    try {
      rawJson = await response.json();
    } catch (e) {
      throw new Error(`Failed to parse response as JSON: ${response.status} ${response.statusText}`);
    }

    const result = OpenAIResponseSchema.safeParse(rawJson);
    if (!result.success) {
      if (rawJson && typeof rawJson === "object" && "error" in rawJson) {
        const errMsg = typeof rawJson.error === "string" ? rawJson.error : (rawJson.error?.message || JSON.stringify(rawJson.error));
        throw new Error(`OpenAI API error: ${errMsg}`);
      }
      throw new Error(`Malformed OpenAI API response: ${result.error.message}`);
    }

    const json = result.data;

    if (json.error) {
      throw new Error(`OpenAI API error: ${json.error.message || JSON.stringify(json.error)}`);
    }

    if (!json.choices || json.choices.length === 0) {
      throw new Error("Unexpected OpenAI API response: Missing 'choices' field.");
    }

    const choice = json.choices[0].message;
    if (!choice) {
      return null;
    }

    let content = choice.content || "";
    if (content && this.anonymizer) {
      content = this.anonymizer.deanonymize(content);
    }

    return {
      role: "assistant",
      content,
      tool_calls: choice.tool_calls ?? undefined,
    };
  }

  /**
   * Performs a streaming completion request.
   * 
   * @param prompt - The prompt to be sent.
   * @param onToken - Callback function invoked for each new content token received.
   * @param tools - Optional tools available for the model to use.
   * @returns A promise resolving to the final consolidated assistant's message.
   */
  async queryStream(
    prompt: IPrompt,
    onToken: (token: string) => void,
    tools?: ToolDefinition[],
    signal?: AbortSignal
  ): Promise<ChatMessage | null> {
    const body = this.createRequestBody(prompt, tools, true);
    log(`OpenAI Request Body: ${JSON.stringify(body, null, 2)}`);

    const response = await this.post(body, signal);

    if (!response.ok) {
      const errText = await response.text();
      throw new Error(`OpenAI API error: ${response.status} - ${errText}`);
    }

    return this.parseStream(response, onToken);
  }

  /**
   * Parses the Server-Sent Events (SSE) stream from the provider's response.
   * 
   * @param response - The IHttpResponse object containing the body stream.
   * @param onToken - Callback function for each content token.
   * @returns A promise resolving to the complete assistant's message after the stream ends.
   * @throws Error if the response body is missing.
   */
  private async parseStream(
    response: IHttpResponse,
    onToken: (token: string) => void
  ): Promise<ChatMessage> {
    if (!response.body) {
      throw new Error("Response body is null");
    }

    const streamingDeanonymizer = this.anonymizer?.createStreamingDeanonymizer();
    let fullContent = "";
    const toolCalls: ToolCall[] = [];
    let buffer = "";

    for await (const chunk of response.body) {
      const { lines, newBuffer } = this.processChunk(chunk.toString(), buffer);
      buffer = newBuffer;

      for (const line of lines) {
        const contentDelta = this.processLine(
          line,
          onToken,
          toolCalls,
          streamingDeanonymizer
        );

        if (contentDelta === null) {
          fullContent += this.flushDeanonymizer(streamingDeanonymizer, onToken);
          return this.createAssistantMessage(fullContent, toolCalls);
        }

        fullContent += contentDelta;
      }
    }

    return this.createAssistantMessage(fullContent, toolCalls);
  }

  /**
   * Processes a chunk of data from the stream, splitting it into lines and updating the buffer.
   * 
   * @param chunk - The new data chunk received from the stream.
   * @param buffer - The current buffer containing the incomplete line from the previous chunk.
   * @returns An object containing the list of complete lines and the new buffer content.
   */
  private processChunk(
    chunk: string,
    buffer: string
  ): { lines: string[]; newBuffer: string } {
    const currentBuffer = buffer + chunk;
    const lines = currentBuffer.split("\n");
    const newBuffer = lines.pop() || "";
    return { lines, newBuffer };
  }

  /**
   * Processes a single line from the SSE stream.
   * 
   * @param line - The line to process.
   * @param onToken - Callback to invoke when a new token is found.
   * @param toolCalls - Array to accumulate tool calls.
   * @param streamingDeanonymizer - The streaming deanonymizer instance, if any.
   * @returns The content delta string to append, or null if the stream has finished ([DONE]).
   */
  private processLine(
    line: string,
    onToken: (token: string) => void,
    toolCalls: ToolCall[],
    streamingDeanonymizer: IStreamingDeanonymizer | undefined
  ): string | null {
    if (!line.startsWith("data: ")) {
      return "";
    }

    const data = line.substring(6).trim();
    if (data === "[DONE]") {
      return null;
    }

    try {
      const rawJson = JSON.parse(data);
      const result = OpenAIStreamChunkSchema.safeParse(rawJson);
      if (!result.success) {
        log(`Malformed stream chunk: ${result.error.message}. Chunk: ${data}`);
        return "";
      }
      const json = result.data;
      const delta = json.choices?.[0]?.delta;
      if (!delta) {
        return "";
      }

      this.handleToolCallsDelta(delta, toolCalls);
      return this.handleContentDelta(delta, onToken, streamingDeanonymizer);
    } catch (e) {
      log("Error parsing chunk: " + data);
      return "";
    }
  }

  /**
   * Processes a content delta from the stream, applying deanonymization if necessary.
   * 
   * @param delta - The delta object from the API response chunk.
   * @param onToken - Callback to invoke with the (potentially deanonymized) token.
   * @param streamingDeanonymizer - The active streaming deanonymizer instance, if any.
   * @returns The processed content token.
   */
  private handleContentDelta(
    delta: OpenAIStreamDelta,
    onToken: (token: string) => void,
    streamingDeanonymizer: IStreamingDeanonymizer | undefined
  ): string {
    if (delta.content) {
      const token = delta.content;
      if (streamingDeanonymizer) {
        const { processed } = streamingDeanonymizer.process(token);
        if (processed) {
          onToken(processed);
          return processed;
        }
      } else {
        onToken(token);
        return token;
      }
    }
    return "";
  }

  /**
   * Accumulates tool call deltas into the provided toolCalls array.
   * 
   * @param delta - The delta object from the API response chunk.
   * @param toolCalls - The array accumulating tool calls for the current response.
   */
  private handleToolCallsDelta(
    delta: OpenAIStreamDelta,
    toolCalls: ToolCall[]
  ): void {
    if (delta.tool_calls) {
      for (const tc of delta.tool_calls) {
        if (!toolCalls[tc.index]) {
          toolCalls[tc.index] = {
            id: tc.id || "",
            type: "function",
            function: { name: "", arguments: "" },
          };
        }
        if (tc.id) {
          toolCalls[tc.index].id = tc.id;
        }
        if (tc.function?.name) {
          toolCalls[tc.index].function.name += tc.function.name;
        }
        if (tc.function?.arguments) {
          toolCalls[tc.index].function.arguments += tc.function.arguments;
        }
      }
    }
  }

  /**
   * Flushes any remaining content from the streaming deanonymizer.
   * 
   * @param streamingDeanonymizer - The active streaming deanonymizer instance.
   * @param onToken - Callback to invoke with the flushed content.
   * @returns The flushed content string.
   */
  private flushDeanonymizer(
    streamingDeanonymizer: IStreamingDeanonymizer | undefined,
    onToken: (token: string) => void
  ): string {
    if (streamingDeanonymizer) {
      const remaining = streamingDeanonymizer.flush();
      if (remaining) {
        onToken(remaining);
        return remaining;
      }
    }
    return "";
  }

  /**
   * Creates the final ChatMessage object for the assistant's response.
   * 
   * @param content - The consolidated content of the message.
   * @param toolCalls - The array of tool calls generated by the model.
   * @returns A ChatMessage object.
   */
  private createAssistantMessage(
    content: string,
    toolCalls: ToolCall[]
  ): ChatMessage {
    return {
      role: "assistant",
      content,
      tool_calls: toolCalls.length > 0 ? toolCalls : undefined,
    };
  }
}