import { z } from "zod";
import { IChatMessage, IAnonymizer, IPrompt, ILlmProvider, ToolDefinition, ToolCall, IStreamingDeanonymizer, IHttpClient, IHttpResponse } from "../types.js";
import { IEventBus } from "../utils/eventBus.js";
import { ToolCallSchema } from "../schemas.js";
import { LLM_MESSAGES, LLM_LOGS } from "../constants/messages.js";
import { createEventLogger } from "../utils/eventLogger.js";

/**
 * Represents the response from a non-streaming OpenAI-compatible completion request.
 */
const OpenAIResponseSchema = z.object({
  /**
   * The list of completion choices returned by the model.
   */
  choices: z.array(
    z.object({
      /**
       * The message generated by the assistant.
       */
      message: z.object({
        /**
         * The text content of the message.
         */
        content: z.string().optional().nullable(),
        /**
         * The reasoning content of the message.
         */
        reasoning: z.string().optional().nullable(),
        /**
         * Alternative name for reasoning content.
         */
        reasoning_content: z.string().optional().nullable(),
        /**
         * Any tool calls requested by the model.
         */
        tool_calls: z.array(ToolCallSchema).nullish(),
      }).nullish(),
    })
  ).optional(),
  /**
   * An error object returned by the API if the request failed.
   */
  error: z.object({
    message: z.string().optional(),
    type: z.string().optional(),
    code: z.union([z.string(), z.number()]).optional(),
  }).optional(),
});

/**
 * Represents a tool call in a streaming response.
 */
const OpenAIStreamingToolCallSchema = z.object({
  /**
   * The index of the tool call in the response array.
   */
  index: z.number().optional().nullable(),
  /**
   * The unique identifier for the tool call.
   */
  id: z.string().optional().nullable(),
  /**
   * The type of the tool call, typically "function".
   */
  type: z.literal("function").optional().nullable(),
  /**
   * The function call details, which may be streamed in parts.
   */
  function: z.object({
    /**
     * The name of the function to be called.
     */
    name: z.string().optional().nullable(),
    /**
     * The arguments for the function, as a JSON string that may be incomplete.
     */
    arguments: z.string().optional().nullable(),
  }).nullish(),
});

/**
 * Represents the incremental change (delta) in a streaming response chunk.
 */
const OpenAIStreamDeltaSchema = z.object({
  /**
   * A partial piece of the assistant's content message.
   */
  content: z.string().optional().nullable(),
  /**
   * A partial piece of the assistant's reasoning.
   */
  reasoning: z.string().optional().nullable(),
  /**
   * Alternative name for reasoning content.
   */
  reasoning_content: z.string().optional().nullable(),
  /**
   * Partial updates to tool calls being generated by the model.
   */
  tool_calls: z.array(OpenAIStreamingToolCallSchema).nullish(),
});

type OpenAIStreamDelta = z.infer<typeof OpenAIStreamDeltaSchema>;

/**
 * Represents a single chunk received from an OpenAI-compatible SSE stream.
 */
const OpenAIStreamChunkSchema = z.object({
  /**
   * The list of choices, each containing a delta update.  
   */
  choices: z.array(
    z.object({
      /**
       * The partial update for this choice.
       */
      delta: OpenAIStreamDeltaSchema.nullish(),
      /**
       * The reason why the model stopped generating tokens.
       */
      finish_reason: z.string().optional().nullable(),
    })
  ).nullish(),
});

/**
 * Represents the body of a completion request sent to an OpenAI-compatible API.
 */
type OpenAIRequestBody = {
  /**
   * The ID of the model to use for the request.
   */
  model: string;
  /**
   * The conversation history to send to the model.
   */
  messages: IChatMessage[];
  /**
   * The maximum number of tokens to generate in the completion.
   */
  max_tokens: number;
  /**
   * Whether to stream back partial progress.
   */
  stream?: boolean;
  /**
   * A list of tools the model may call.
   */
  tools?: {
    /**
     * The type of the tool.
     */
    type: "function";
    /**
     * The definition of the function tool.
     */
    function: ToolDefinition;
  }[];
};

/**
 * Arguments for the OpenAICompatibleProvider constructor.
 */
export interface IOpenAICompatibleProviderArgs {
  /** The HTTP client to use for requests. */
  httpClient: IHttpClient;
  /** The API endpoint URL for the OpenAI-compatible service. */
  endpoint: string;
  /** The API key for authentication. */
  apiKey: string;
  /** The model identifier to be used for completions. */
  model: string;
  /** The event bus to emit token events. */
  eventBus: IEventBus;
  /** Optional anonymizer to protect sensitive data in user messages. */
  anonymizer?: IAnonymizer;
}

export class OpenAICompatibleProvider implements ILlmProvider {
  private httpClient: IHttpClient;
  private endpoint: string;
  private apiKey: string;
  private model: string;
  private eventBus: IEventBus;
  private anonymizer?: IAnonymizer;

  private logger: ReturnType<typeof createEventLogger>;

  /**
   * Creates an instance of OpenAICompatibleProvider.
   * 
   * @param args - The configuration arguments for the provider.
   */
  constructor({
    httpClient,
    endpoint,
    apiKey,
    model,
    eventBus,
    anonymizer,
  }: IOpenAICompatibleProviderArgs) {
    this.httpClient = httpClient;
    this.endpoint = endpoint;
    this.apiKey = apiKey;
    this.model = model;
    this.eventBus = eventBus;
    this.anonymizer = anonymizer;
    this.logger = createEventLogger(eventBus);
  }

  /**
   * Prepares chat messages for the API request, applying anonymization to user messages if configured.
   * 
   * @param conversation - The list of chat messages in the conversation history.
   * @returns An array of message objects formatted for the OpenAI-compatible API.
   */
  private prepareMessages(
    conversation: IChatMessage[]
  ): IChatMessage[] {
    return conversation.map((message) => {
      const role = message.role;

      let content = message.content;
      if (this.anonymizer && message.role === "user") {
        content = this.anonymizer.anonymize(message.content);
      }

      const msg: IChatMessage = {
        role,
        content,
        reasoning: message.reasoning,
        tool_calls: message.tool_calls,
        tool_call_id: message.tool_call_id
      };
      return msg;
    });
  }

  /**
   * Constructs the request body for the completion API.
   * 
   * @param prompt - The prompt object that generates the chat history.
   * @param tools - Optional tool definitions for function calling.
   * @param stream - Whether the response should be streamed.
   * @returns A JSON-serializable object representing the request body.
   */
  private createRequestBody(
    prompt: IPrompt,
    tools: ToolDefinition[] | undefined,
    stream: boolean
  ): OpenAIRequestBody {
    const conversation = prompt.generateChatHistory();
    const messages = this.prepareMessages(conversation);

    const body: OpenAIRequestBody = {
      model: this.model,
      messages: messages,
      max_tokens: 10000,
    };

    if (stream) {
      body.stream = true;
    }

    if (tools && tools.length > 0) {
      body.tools = tools.map((t) => ({ type: "function", function: t }));
    }

    return body;
  }

  /**
   * Sends a POST request to the provider's endpoint using the injected httpClient.
   * 
   * @param body - The request body to be sent as JSON.
   * @param signal - Optional AbortSignal to cancel the request.
   * @returns A promise that resolves to the IHttpResponse object.
   */
  private async post(body: OpenAIRequestBody, signal?: AbortSignal): Promise<IHttpResponse> {
    return await this.httpClient.post(this.endpoint, {
      headers: {
        Authorization: `Bearer ${this.apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
      signal,
    });
  }

  /**
   * Performs a non-streaming completion request.
   * 
   * @param prompt - The prompt to be sent.
   * @param tools - Optional tools available for the model to use.
   * @param signal - Optional AbortSignal to cancel the request.
   * @returns A promise resolving to the assistant's message, or null if no choice was returned.
   */
  async query(
    prompt: IPrompt,
    tools?: ToolDefinition[],
    signal?: AbortSignal
  ): Promise<IChatMessage | null> {
    const body = this.createRequestBody(prompt, tools, false);
    const response = await this.post(body, signal);

    let rawJson: any;
    try {
      rawJson = await response.json();
    } catch (e) {
      throw new Error(LLM_MESSAGES.PARSE_JSON_FAILED(response.status, response.statusText));
    }

    const result = OpenAIResponseSchema.safeParse(rawJson);
    if (!result.success) {
      if (rawJson && typeof rawJson === "object" && "error" in rawJson) {
        const errMsg = typeof rawJson.error === "string" ? rawJson.error : (rawJson.error?.message || JSON.stringify(rawJson.error));
        throw new Error(LLM_MESSAGES.OPENAI_GENERIC_ERROR(errMsg));
      }
      throw new Error(LLM_MESSAGES.MALFORMED_RESPONSE(result.error.message));
    }

    const json = result.data;

    if (json.error) {
      throw new Error(LLM_MESSAGES.OPENAI_GENERIC_ERROR(json.error.message || JSON.stringify(json.error)));
    }

    if (!json.choices || json.choices.length === 0) {
      throw new Error(LLM_MESSAGES.MISSING_CHOICES);
    }

    const choice = json.choices[0].message;
    if (!choice) {
      return null;
    }

    let content = choice.content || "";
    if (content && this.anonymizer) {
      content = this.anonymizer.deanonymize(content);
    }

    return {
      role: "assistant",
      content,
      reasoning: choice.reasoning || choice.reasoning_content || undefined,
      tool_calls: choice.tool_calls ?? undefined,
    };
  }

  /**
   * Performs a streaming completion request.
   * 
   * @param prompt - The prompt to be sent.
   * @param tools - Optional tools available for the model to use.
   * @returns A promise resolving to the final consolidated assistant's message.
   */
  async queryStream(
    prompt: IPrompt,
    tools?: ToolDefinition[],
    signal?: AbortSignal
  ): Promise<IChatMessage | null> {
    const body = this.createRequestBody(prompt, tools, true);
    this.logger.debug(`OpenAI Request Body: ${JSON.stringify(body, null, 2)}`);

    const response = await this.post(body, signal);

    if (!response.ok) {
      const errText = await response.text();
      throw new Error(LLM_MESSAGES.OPENAI_ERROR(response.status, errText));
    }

    this.logger.info(LLM_LOGS.RECEIVING_STREAM);
    const result = await this.parseStream(response);
    this.logger.debug(`Consolidated content length: ${result.content.length}`);
    if (result.reasoning) {
      this.logger.debug(`Consolidated reasoning length: ${result.reasoning.length}`);
    }
    return result;
  }

  /**
   * Parses the Server-Sent Events (SSE) stream from the provider's response.
   * 
   * @param response - The IHttpResponse object containing the body stream.
   * @returns A promise resolving to the complete assistant's message after the stream ends.
   * @throws Error if the response body is missing.
   */
  private async parseStream(
    response: IHttpResponse
  ): Promise<IChatMessage> {
    if (!response.body) {
      throw new Error(LLM_MESSAGES.RESPONSE_BODY_NULL);
    }

    const streamingDeanonymizer = this.anonymizer?.createStreamingDeanonymizer();
    let fullContent = "";
    let fullReasoning = "";
    const toolCalls: ToolCall[] = [];
    let buffer = "";

    for await (const chunk of response.body) {
      const chunkStr = chunk.toString();
      this.logger.debug(LLM_LOGS.STREAM_CHUNK_RECEIVED(chunkStr.length));
      const { lines, newBuffer } = this.processChunk(chunkStr, buffer);
      buffer = newBuffer;

      for (const line of lines) {
        const deltaResult = this.processLine(
          line,
          toolCalls,
          streamingDeanonymizer
        );

        if (deltaResult === null) {
          this.logger.debug(LLM_LOGS.STREAM_DONE);
          fullContent += this.flushDeanonymizer(streamingDeanonymizer);
          return this.createAssistantMessage(fullContent, fullReasoning, toolCalls);
        }

        fullContent += deltaResult.content;
        fullReasoning += deltaResult.reasoning;
      }
    }

    this.logger.info(LLM_LOGS.STREAM_FINISHED);
    fullContent += this.flushDeanonymizer(streamingDeanonymizer);
    return this.createAssistantMessage(fullContent, fullReasoning, toolCalls);
  }

  /**
   * Processes a chunk of data from the stream, splitting it into lines and updating the buffer.
   * 
   * @param chunk - The new data chunk received from the stream.
   * @param buffer - The current buffer containing the incomplete line from the previous chunk.
   * @returns An object containing the list of complete lines and the new buffer content.
   */
  private processChunk(
    chunk: string,
    buffer: string
  ): { lines: string[]; newBuffer: string } {
    const currentBuffer = buffer + chunk;
    const lines = currentBuffer.split("\n");
    const newBuffer = lines.pop() || "";
    return { lines, newBuffer };
  }

  /**
   * Processes a single line from the SSE stream.
   * 
   * @param line - The line to process.
   * @param toolCalls - Array to accumulate tool calls.
   * @param streamingDeanonymizer - The streaming deanonymizer instance, if any.
   * @returns The content and reasoning deltas, or null if the stream has finished ([DONE]).
   */
  private processLine(
    line: string,
    toolCalls: ToolCall[],
    streamingDeanonymizer: IStreamingDeanonymizer | undefined
  ): { content: string; reasoning: string } | null {
    if (!line.startsWith("data: ")) {
      return { content: "", reasoning: "" };
    }

    const data = line.substring(6).trim();
    this.logger.debug(LLM_LOGS.STREAM_DATA_RECEIVED(data));
    if (data === "[DONE]") {
      return null;
    }

    try {
      const rawJson = JSON.parse(data);
      const result = OpenAIStreamChunkSchema.safeParse(rawJson);
      if (!result.success) {
        this.logger.error(LLM_MESSAGES.MALFORMED_RESPONSE(`${result.error.message}. Chunk: ${data}`));
        return { content: "", reasoning: "" };
      }
      const json = result.data;
      const choice = json.choices?.[0];
      const delta = choice?.delta;

      if (choice?.finish_reason) {
        this.logger.debug(LLM_LOGS.STREAM_FINISH_REASON(choice.finish_reason));
      }

      if (!delta) {
        return { content: "", reasoning: "" };
      }

      this.handleToolCallsDelta(delta, toolCalls);
      return this.handleContentDelta(delta, streamingDeanonymizer);
    } catch (e) {
      this.logger.error(LLM_MESSAGES.PARSE_CHUNK_ERROR(data));
      return { content: "", reasoning: "" };
    }
  }

  /**
   * Processes a content delta from the stream, applying deanonymization if necessary.
   * 
   * @param delta - The delta object from the API response chunk.
   * @param streamingDeanonymizer - The active streaming deanonymizer instance, if any.
   * @returns The processed content and reasoning tokens.
   */
  private handleContentDelta(
    delta: OpenAIStreamDelta,
    streamingDeanonymizer: IStreamingDeanonymizer | undefined
  ): { content: string; reasoning: string } {
    let content = "";
    let reasoning = "";

    if (delta.reasoning || delta.reasoning_content) {
      const token = (delta.reasoning || delta.reasoning_content)!;
      this.eventBus.emit('agent:token', { token, type: 'reasoning' });
      reasoning = token;
    }

    if (delta.content) {
      const token = delta.content;
      if (streamingDeanonymizer) {
        const { processed } = streamingDeanonymizer.process(token);
        if (processed) {
          this.eventBus.emit('agent:token', { token: processed, type: 'content' });
          content = processed;
        }
      } else {
        this.eventBus.emit('agent:token', { token, type: 'content' });
        content = token;
      }
    }
    return { content, reasoning };
  }

  /**
   * Accumulates tool call deltas into the provided toolCalls array.
   * 
   * @param delta - The delta object from the API response chunk.
   * @param toolCalls - The array accumulating tool calls for the current response.
   */
  private handleToolCallsDelta(
    delta: OpenAIStreamDelta,
    toolCalls: ToolCall[]
  ): void {
    if (delta.tool_calls) {
      for (const tc of delta.tool_calls) {
        if (tc.index === undefined || tc.index === null) {
          continue;
        }
        if (!toolCalls[tc.index]) {
          toolCalls[tc.index] = {
            id: tc.id || "",
            type: "function",
            function: { name: "", arguments: "" },
          };
        } else if (tc.id && toolCalls[tc.index].id && toolCalls[tc.index].id !== tc.id) {
          // If we see a new ID for an existing index, it means the model is starting a new tool call
          // at the same index (non-standard behavior from some providers). 
          // We reset the function name and arguments to avoid merging them.
          toolCalls[tc.index].id = tc.id;
          toolCalls[tc.index].function.name = "";
          toolCalls[tc.index].function.arguments = "";
        }

        if (tc.id) {
          toolCalls[tc.index].id = tc.id;
        }
        if (tc.function?.name) {
          toolCalls[tc.index].function.name += tc.function.name;
        }
        if (tc.function?.arguments) {
          toolCalls[tc.index].function.arguments += tc.function.arguments;
        }
      }
    }
  }

  /**
   * Flushes any remaining content from the streaming deanonymizer.
   * 
   * @param streamingDeanonymizer - The active streaming deanonymizer instance.
   * @returns The flushed content string.
   */
  private flushDeanonymizer(
    streamingDeanonymizer: IStreamingDeanonymizer | undefined
  ): string {
    if (streamingDeanonymizer) {
      const remaining = streamingDeanonymizer.flush();
      if (remaining) {
        this.eventBus.emit('agent:token', { token: remaining, type: 'content' });
        return remaining;
      }
    }
    return "";
  }

  /**
   * Creates the final ChatMessage object for the assistant's response.
   * 
   * @param content - The consolidated content of the message.
   * @param reasoning - The consolidated reasoning of the message.
   * @param toolCalls - The array of tool calls generated by the model.
   * @returns A ChatMessage object.
   */
  private createAssistantMessage(
    content: string,
    reasoning: string,
    toolCalls: ToolCall[]
  ): IChatMessage {
    return {
      role: "assistant",
      content,
      reasoning: reasoning || undefined,
      tool_calls: toolCalls.length > 0 ? toolCalls : undefined,
    };
  }
}
